{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyIuc57zyNZK"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Se está siguiendo como referencia la guía [*Train a diffusion model*](https://huggingface.co/docs/diffusers/en/tutorials/basic_training#train-a-diffusion-model) de Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8iztztvyNZO"
      },
      "source": [
        "Para evitar la repetición de código y agilizar las partes iterativas del entrenamiento, se crea una clase con los hiperparámetros del modelo a crear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLHlhMJdyNZR"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = 128\n",
        "    train_batch_size = 8\n",
        "    eval_batch_size = 16\n",
        "    num_epochs = 50\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 10\n",
        "    save_model_epochs = 30\n",
        "    mixed_precision = \"fp16\"\n",
        "    seed = 42\n",
        "    output_dir = \"ddpm-cars-128\"\n",
        "\n",
        "    push_to_hub = False\n",
        "\n",
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87NRvzomyNZU"
      },
      "source": [
        "# Preparación\n",
        "\n",
        "Esta fase se conforma de los siguientes pasos:\n",
        "\n",
        "- Descargar el dataset\n",
        "- Realizar la división 70-30.\n",
        "- Normalizar imágenes\n",
        "\n",
        "A continuación, se descarga el dataset (`tanganke/stanford_cars`)[https://huggingface.co/datasets/tanganke/stanford_cars] de Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb9K8y2XyNZV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "config.dataset_name = \"tanganke/stanford_cars\"\n",
        "dataset = load_dataset(config.dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bywbohvDyNZX"
      },
      "source": [
        "Se extrae una muestra del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q12nAIQbyNZY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"image\"]):\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDoQMKU4yNZZ"
      },
      "source": [
        "Se define la función de preprocesamiento de las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pAZh5EIyNZa"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((config.image_size, config.image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqVWQ03xyNZc"
      },
      "source": [
        "Se da una muestra de cómo quedaría una imagen después del preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMBlX1QWyNZd"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d_tVZd7yNZe"
      },
      "source": [
        "Se envuelve el dataset en un `DataLoader`, que permite cargar el dataset desde múltiples hilos y aleatorizar el orden de las imágenes empleadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB8POnw0yNZf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31GztGXmyNZg"
      },
      "source": [
        "# Creación del modelo\n",
        "\n",
        "En este ejemplo, se usará una UNet para la generación de imágenes. Se darán 3 canales de entrada y salida, cada uno correspondiendo a un color del espacio RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmd5irV2yNZg"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjrMnh74yNZh"
      },
      "source": [
        "He de notar algo importante acá. Recordando el funcionamiento de una UNet, la capa de *downsampling*, es decir, el *encoder*, va a generalizar características sobre la imagen que se esté tratando. De hecho, es una red neuronal convolucional (CNN), lo que ya debería ser familiar en este momento.\n",
        "\n",
        "Lo definido en `block_out_channels=(128, 128, 256, 256, 512, 512)` corresponde a los kernels que se aplican en la entrada de cada bloque. La cantidad de elementos en la tupla son la cantidad de kernels aplicados, mientras que cada valor corresponde a la cantidad de kernels que se aplica en cada capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvCD6Y4vyNZi"
      },
      "outputs": [],
      "source": [
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(\"Input shape:\", sample_image.shape)\n",
        "\n",
        "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkipHGVTyNZi"
      },
      "source": [
        "El componente que se encarga de la adición y limpieza de ruido es el scheduler. El scheduler tiene dos funciones principales:\n",
        "\n",
        "1. Se encarga de añadir ruido de manera proporcional siguiendo una cantidad de pasos dada. Diferentes schedulers usarán distribuciones diferentes para el ruido. En esta ocasión se usará [DDPMScheduler](https://huggingface.co/docs/diffusers/en/api/schedulers/ddpm), donde el ruido se distribuye de manera gaussiana y se ajusta en cada paso hacia adelante (fast-forward).\n",
        "    \n",
        "    Este scheduler se basa en cadenas de Markov. En las cadenas de Markov, el estado actual sólo depende del estado anterior, no de la secuencias que lo condujeron al estado actual. En este contexto, cada imagen ruidosa $x_i$ es un estado en la cadena, por lo que su estado depende sólo de la imagen anterior $x_{i-1}$. De este modo, a medida de que se agregan más pasos de ruidos, se transita por una cadena de estados donde el estado final es una imagen conformada por puro ruido.\n",
        "2. Luego de la fase de encoding, el scheduler indica cuánto ruido debe eliminarse siguiendo una regla aprendida durante el entrenamiento, limpiando la imagen, generándola de este modo. De nuevo, se emplea el concepto de cadenas de Markov, donde el estado de una imagen con menor ruido $y_i$ es el resultado de una imagen anterior con más ruido $y_{i-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAVE4E-gyNZj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGaTurpNyNZj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "noise_pred = model(noisy_image, timesteps).sample\n",
        "loss = F.mse_loss(noise_pred, noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuZ8C9TJyNZk"
      },
      "source": [
        "Para el entrenamiento, se utilizará el optimizador AdamW, el cual es una variante de Adam adaptada para realizar decaimiento del peso, lo que evita el sobreajuste y ayuda a regularizar mejor el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIPx8EICyNZk"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3uE0_R-yNZl"
      },
      "source": [
        "Cada 10 épocas, se guarda un grid de imágenes generadas con base a unas imágenes del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkVphFkyyNZl"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "from diffusers.utils import make_image_grid\n",
        "import os\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kTInSFhyNZl"
      },
      "source": [
        "Mediante la librería Accelerator se define todo el flujo de trabajo de la generación de imágenes. Se cargan los hiperparámetros definidos al inicio, y se crea un bucle donde se va a entrenar el modelo hasta las épocas dadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f4AJbMryNZl"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        if config.push_to_hub:\n",
        "            repo_id = create_repo(\n",
        "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
        "            ).repo_id\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[\"images\"]\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
        "                dtype=torch.int64\n",
        "            )\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                if config.push_to_hub:\n",
        "                    upload_folder(\n",
        "                        repo_id=repo_id,\n",
        "                        folder_path=config.output_dir,\n",
        "                        commit_message=f\"Epoch {epoch}\",\n",
        "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        "                    )\n",
        "                else:\n",
        "                    pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbrqAA1zyNZm"
      },
      "source": [
        "La función `notebook_launcher` se encarga de ejecutar el bucle de entrenamiento, repartiendo la carga entre el hardware disponible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psrOUw7ByNZn"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ze0BZHCyNZn"
      },
      "source": [
        "Se guarda el estado del optimizador y del scheduler para seguir entrenando el modelo más tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10cd7wbdyNZn"
      },
      "outputs": [],
      "source": [
        "output_dir = config.output_dir\n",
        "\n",
        "# Guardar el estado del optimizador\n",
        "optimizer_state_path = os.path.join(output_dir, \"optimizer_state.pt\")\n",
        "torch.save(optimizer.state_dict(), optimizer_state_path)\n",
        "\n",
        "# Guardar el estado del scheduler\n",
        "scheduler_state_path = os.path.join(output_dir, \"scheduler_state.pt\")\n",
        "torch.save(lr_scheduler.state_dict(), scheduler_state_path)\n",
        "\n",
        "print(f\"Optimizer and scheduler states saved at {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}